{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5icR4XCZgm4O"
      },
      "source": [
        "# Preprocess on all data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Drop null values\n",
        "- Preprocess texts (tokenization, lemmatization, stop-word removal)."
      ],
      "metadata": {
        "id": "IypYLnGZ1Cft"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XGUAJ5aG1Q-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ojRhj0Oxfxo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dbcba71-ff3f-4bb1-83f2-aa4f7aa27ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the paths\n",
        "f = \"/content/drive/MyDrive/the-reddit-climate-change-dataset-comments.csv\" #original data\n",
        "output = \"/content/drive/MyDrive/all_data.csv\" #output file"
      ],
      "metadata": {
        "id": "XUxBEEDjBhBh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages imported\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "3aLRyOAoIXDp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(f)\n",
        "c = df[['id','body','sentiment']]"
      ],
      "metadata": {
        "id": "e7bLFUq0wHyG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['sentiment'],\n",
        "axis=0, \n",
        "how='any', \n",
        "inplace=True \n",
        ")"
      ],
      "metadata": {
        "id": "XCJ7W44v0o7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text processing (tokenization, stop words removal)\n",
        "def preProcess(doc):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    doc : List of String\n",
        "        A document to process.\n",
        "    Returns the processed document : [0]: a list of words from the basic sentences; [1]: list of tokenized words; [2]: a string of of tokenized words.\n",
        "    -------\n",
        "    TYPE\n",
        "        Process and return a document (tokenization, lemmatization, stop-word removal).\n",
        "    \"\"\"\n",
        "        \n",
        "    Stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    WN_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    sentences = sent_tokenize(doc)\n",
        "    tokens = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        words = [stemmer.stem(word) for word in words]\n",
        "        words = [WN_lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
        "\n",
        "        words = [word for word in words if word.isalpha() and word not in Stopwords]  # get rid of numbers and Stopwords\n",
        "        tokens.extend(words)\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-BJnB0ywH0y",
        "outputId": "970fcafa-a6d8-4f4b-e8c7-70b64dd6d021"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c['c_body'] = c.apply(lambda row : preProcess(row['body']), axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyMQCRk5wH3K",
        "outputId": "5f7ab7b4-bc7b-46cb-f37c-d03a788f9650"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-6d3de48c9535>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  c['c_body'] = c.apply(lambda row : preProcess(row['body']), axis = 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c.dropna(subset=['sentiment'],\n",
        "axis=0, \n",
        "how='any', \n",
        "inplace=True \n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXaOyWVPL6dW",
        "outputId": "307bc1ec-9189-4389-9187-015f30346308"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-73bf6a60bbed>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  c.dropna(subset=['sentiment','c_body'],\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['body_cleaned'] = c['c_body']"
      ],
      "metadata": {
        "id": "1OFER-T5ugTi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = df.drop(['body'],axis=1)"
      ],
      "metadata": {
        "id": "iNRqCmX0umML"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlV9jfB9vQ2r",
        "outputId": "0d018080-a207-47fa-ce53-66e52bf5ea44"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['type', 'id', 'subreddit.id', 'subreddit.name', 'subreddit.nsfw',\n",
              "       'created_utc', 'permalink', 'sentiment', 'score', 'body_cleaned'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.to_csv(output,index=False,encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "1bwx7nvxvbhQ"
      },
      "execution_count": 34,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}